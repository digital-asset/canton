# Release of Canton 2.9.5

Canton 2.9.5 has been released on October 22, 2024. You can download the Daml Open Source edition from the Daml Connect [Github Release Section](https://github.com/digital-asset/daml/releases/tag/v2.9.5). The Enterprise edition is available on [Artifactory](https://digitalasset.jfrog.io/artifactory/canton-enterprise/canton-enterprise-2.9.5.zip).
Please also consult the [full documentation of this release](https://docs.daml.com/2.9.5/canton/about.html).

## Summary

This is a maintenance release of Canton that fixes bugs, including two critical bugs that can corrupt the state of a participant node when retroactive interfaces or migrated contracts from protocol version 3 are used.

## Bugfixes

### (24-020, Critical): Participant crashes due to retroactive interface validation

#### Description
The view reinterpreation of an exercise of a retroactive interface may fail because the engine does not explicitliy request the interface package. This can lead to a ledger fork as participants come to different conclusions.

#### Affected Deployments

Participant

#### Affected Versions
2.5, 2.6, 2.7, 2.8.0-2.8.9, 2.9.0-2.9.4

#### Impact

A participant crashes during transaction validation when using retroactive interfaces.

#### Symptom

"Validating participant emits warning:

#### Workaround

None

#### Likeliness

Very likely for all multi participant setups that uses retroactive interface instances.

#### Recommendation

Upgrade to 2.9.5

```

LOCAL_VERDICT_FAILED_MODEL_CONFORMANCE_CHECK(5,571d2e8a): Rejected transaction due to a failed model conformance check: DAMLeError(
  Preprocessing(
    Lookup(
      NotFound(
        Package(
```
And then emits an error:
```
An internal error has occurred.
java.lang.IllegalStateException: Mediator approved a request that we have locally rejected
```

#### Workaround

None

#### Likeliness

Very likely for all multi participant setups that uses retroactive interface instances.

#### Recommendation

Upgrade to 2.9.5

### (24-024, Critical): Participant incorrectly handles unauthenticated contract IDs in PV5

#### Issue Description

Contracts created on participants running PV3 have an unauthenticated contract ID. When these participants are upgraded to PV5 **without setting the `allow-for-unauthenticated-contract-ids` flag to true**, any submitted transaction that uses such unauthenticated contract IDs will produce warnings during validation, but also put the participants in an incorrect state. From then on, the participant will not output any ledger events any more and fail to reconnect to the domain.

#### Affected Deployments

Participant

#### Affected Versions
2.9.0-2.9.4

#### Impact

The participant is left in a failed state.

#### Symptom

Connecting to the domain fails with an internal error `IllegalStateException: Cannot find event for sequenced in-flight submission`.

The participant does not emit any ledger events any more.

#### Workaround

No workaround by clients possible. Support and engineering can try to fix the participants by modifying the participant's database tables.

#### Likeliness

Needs a submission request using a contract with unauthenticated contract ID. This can only happen for participants who have been migrated from using PV3 to PV5, and have not set the flag to allow unauthenticated contracts on all involved participants.

#### Recommendation

Upgrade during the next maintenance window to a version with the fix.
If an upgrade is not possible and old contracts from PV3 are used, enable the `allow-for-unauthenticated-contract-ids` flag on all the participants.

### (24-026, High): Hard Synchronization Domain Migration fails to check for in-flight transactions

#### Issue Description

Since 2.9.0, the Hard Synchronization Domain Migration command `repair.migrate_domain` aborts when it detects in-flight submissions on the participant. However, it should also check for in-flight transactions.

#### Affected Deployments

Participant

#### Affected Versions

2.9.0-2.9.4

#### Impact

Performing a Hard Synchronization Domain Migration while there are still in-flight submissions and transactions may result in a ledger-fork.

#### Symptom

Ledger-fork after running the Hard Synchronization Domain Migration command `repair.migrate_domain` that may result in ACS commitment mismatches.

#### Workaround

Follow the documented steps, in particular ensure that there is no activity on all participants before proceeding with a Hard Synchronization Domain Migration.

#### Likeliness

The bug only manifests when the operator skips the documented step for the Hard Synchronization Domain Migration to ensure that there is no activity on all participants anymore in combination with still having in-flight transactions when the migration executes.

#### Recommendation

Upgrade to 2.9.5 to properly safe-guard against running the Hard Synchronization Domain Migration command `repair.migrate_domain` while there are still in-flight submissions or transactions.


### (24-021, Medium): Participant replica fails to become active

#### Issue Description

A participant replica fails to become active under certain database network conditions. The previously active replica fails to fully transition to passive due to blocked database connection health checks, which leaves the other replica to fail to transition to active. Eventually the database health checks get unblocked and the replica transitions to passive, but the other replica does not recover from the previous active transition failure, which leaves both replicas passive.

#### Affected Deployments

Participant

#### Affected Versions

All 2.3-2.7
2.8.0-2.8.10
2.9.0-2.9.4

#### Impact

Both participant replicas remain passive and do not serve transactions.

#### Symptom

The transition to active failed on a participant due to maximum retries exhausted:
```
2024-09-02T07:08:56,178Z participant2 [c.d.c.r.DbStorageMulti:participant=participant1] [canton-env-ec-36] ERROR dd:[ ] c.d.c.r.DbStorageMulti:participant=participant1 tid:effa59a8f7ddec2e132079f2a4bd9885 - Failed to transition replica state
com.daml.timer.RetryStrategy$TooManyAttemptsException: Gave up trying after Some(3000) attempts and 300.701142545 seconds.
```

#### Workaround

Restart both replicas of the participant

#### Likeliness

Possible under specific database connection issues

#### Recommendation

Upgrade to the next patch release during regular maintenance window.

### (24-022, Medium): Participant replica does not clear package service cache

#### Issue Description

When a participant replica becomes active, it does not refresh its package service cache. If a vetting attempt is made on the participant that fails because the package is not uploaded, the "missing package" response is cached. If the package is then uploaded to another replica, and we switch to the original participant, this package service cache will still record the package as nonexistent. When the package is used in a transaction, we will get a local model conformance error as the transaction validator cannot find the package, whereas other parts of the participant that don't use the package service can successfully locate it.

#### Affected Deployments

Participant

#### Affected Versions
2.8.0-2.8.10, 2.9.0-2.9.4

#### Impact

Replica crashes during transaction validation.

#### Symptom

Validating participant emits warning:
```

LOCAL_VERDICT_FAILED_MODEL_CONFORMANCE_CHECK(5,a2b60642): Rejected transaction due to a failed model conformance check: UnvettedPackages
```
And then emits an error:
```
An internal error has occurred.
java.lang.IllegalStateException: Mediator approved a request that we have locally rejected
```

#### Workaround

Restart recently active replica

#### Likeliness

Likely to happen in any replicated participant setup with frequent vetting attempts and switches between active and passive replicated participants between those vetting attempts.

#### Recommendation

Users are advised to upgrade to the next patch release (2.9.5) during their maintenance window.

### (24-023, Low): Participant fails to start if quickly acquiring and then losing DB connection during bootstrap

#### Issue Description

When a participant starts up and acquires the active lock, the participant replica initializes its storage and begins its bootstrap logic. If during the bootstrap logic and before the replica attempts to initializate its identity, the replica loses the DB connection, bootstrapping will be halted until its identity is initialized by another replica or re-acquires the lock. When the lock is lost, the replica manager will attempt to transition the participant state to passive, which assumes the participant has been initialized fully, which in this case it hasn't. Therefore the passive transition waits indefinitely.

#### Affected Deployments

Participant

#### Affected Versions
2.8.0-2.8.10, 2.9.0-2.9.4

#### Impact

Replica gets stuck transitioning to passive state during bootstrap.

#### Symptom

Participant keeps emitting info logs as follows indefinitely

```
Replica state update to Passive has not completed after
```

#### Workaround

Restart the node

#### Likeliness

Exceptional, requires acquiring then losing the DB connection with a precise timing during bootstrap of the node.

#### Recommendation

Users are advised to upgrade to the next patch release (2.9.5) during their maintenance window.

### (24-025, Low): Commands for single key rotation for sequencer and mediator node fail

#### Description
The current commands for single key rotation with sequencer and mediator nodes (`rotate_node_key`
and `rotate_kms_node_key`) fail because they do not have the necessary domain manager reference needed to find
the old key and export the new key.

#### Affected Deployments

Sequencer and mediator nodes

#### Affected Versions
All 2.3-2.7, 2.8.0-2.8.10, 2.9.0-2.9.4

#### Impact

Key rotation for individual keys with sequencer or mediator nodes cannot be performed using the provided commands.

#### Symptom

Current single key rotation for sequencer and mediator, with commands `rotate_node_key` and
`rotate_kms_node_key`, fails with an `IllegalStateException: key xyz does not exist`.

#### Workaround

Use the domain manager to rotate a mediator or sequencer key, or use the `rotate_node_keys` command
with a domain manager reference to rotate all keys.

#### Likeliness

This issue consistently occurs when trying to rotate keys individually with sequencer or mediator nodes in
a distributed environment.

#### Recommendation

Upgrade to 2.9.5 when affected, and run the `rotate_node_key` and `rotate_kms_node_key` commands with a reference to the
domain topology manager to successfully perform the rotation.

### (24-027, Low): Bootstrap of the domain fails if the mediator or sequencer share the same key as the domain manager

#### Description
Domain bootstrapping fails with a `KeyAlreadyExists` error when the signing key is shared between the mediator/sequencer
and the domain manager.

#### Affected Deployments

Sequencer and mediator nodes

#### Affected Versions
All 2.3-2.7, 2.8.0-2.8.10, 2.9.0-2.9.4

#### Impact

You cannot bootstrap a domain when the signing key is shared between the domain manager and mediator or sequencer nodes.

#### Symptom

After calling `bootstrap_domain` we get a `KeyAlreadyExists` error.

#### Workaround

Use different signing keys for the mediator, sequencer and the domain manager.

#### Likeliness

This issue consistently occurs whenever we attempt to bootstrap a domain where the domain manager's signing key is shared with the mediator or the sequencer.

#### Recommendation

Upgrade to 2.9.5 when affected by this limitation.


## Compatibility

The following Canton protocol and Ethereum sequencer contract versions are supported:

| Dependency                 | Version                    |
|----------------------------|----------------------------|
| Canton protocol versions   | 5          |

Canton has been tested against the following versions of its dependencies:

| Dependency                 | Version                    |
|----------------------------|----------------------------|
| Java Runtime               | OpenJDK 64-Bit Server VM Zulu11.72+19-CA (build 11.0.23+9-LTS, mixed mode)               |
| Postgres                   | Recommended: PostgreSQL 12.20 (Debian 12.20-1.pgdg120+1) â€“ Also tested: PostgreSQL 11.16 (Debian 11.16-1.pgdg90+1), PostgreSQL 13.16 (Debian 13.16-1.pgdg120+1), PostgreSQL 14.13 (Debian 14.13-1.pgdg120+1), PostgreSQL 15.8 (Debian 15.8-1.pgdg120+1)           |
| Oracle                     | 19.20.0             |


